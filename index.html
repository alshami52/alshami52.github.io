<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ali K. AlShami - Personal Website</title>
    <link rel="stylesheet" href="assets/css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:wght@400;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="index.html" class="nav-logo">AKA</a>
            <ul class="nav-menu" id="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="publications.html" class="nav-link">Publications</a></li>
                <li><a href="talks.html" class="nav-link">Talks</a></li>
                <li><a href="professional-appointments.html" class="nav-link">Professional Appointments</a></li>
                <li><a href="academic-service.html" class="nav-link">Leadership &amp; Collaboration &amp; Services</a></li>
                <li><a href="research.html" class="nav-link">Research</a></li>
                <li><a href="awards.html" class="nav-link">Awards</a></li>
            </ul>
            <div class="hamburger" id="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
    </nav>

    <!-- Fixed background (stays visible when scrolling) -->
    <div class="hero-background hero-background--fixed">
        <div class="gradient-orb orb-1"></div>
        <div class="gradient-orb orb-2"></div>
        <div class="gradient-orb orb-3"></div>
    </div>

    <!-- Main layout: sticky left (photo + links), scrolling right (bio + interests + news + footer) -->
    <div class="main-layout">
        <aside class="sidebar-sticky">
            <div class="hero-image">
                <div class="image-wrapper">
                    <img src="assets/img/portrait.jpeg" alt="Ali K. AlShami" class="portrait-img">
                    <div class="image-glow"></div>
                </div>
                <ul class="profile-links">
                    <li>
                        <span class="profile-icon" aria-hidden="true">
                            <svg viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C8.13 2 5 5.13 5 9c0 5.25 7 13 7 13s7-7.75 7-13c0-3.87-3.13-7-7-7zm0 9.5c-1.38 0-2.5-1.12-2.5-2.5s1.12-2.5 2.5-2.5 2.5 1.12 2.5 2.5-1.12 2.5-2.5 2.5z"/></svg>
                        </span>
                        <span>San Francisco Silicon Valley</span>
                    </li>
                    <li>
                        <a href="mailto:aalshami@uccs.edu">
                            <span class="profile-icon" aria-hidden="true">
                                <svg viewBox="0 0 24 24" fill="currentColor"><path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/></svg>
                            </span>
                            <span>aalshami@uccs.edu</span>
                        </a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/ali-k-alshami-bb81a5a4/" target="_blank" rel="noopener noreferrer">
                            <span class="profile-icon" aria-hidden="true">
                                <svg viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
                            </span>
                            <span>LinkedIn</span>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=Vv1FePkAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">
                            <span class="profile-icon" aria-hidden="true">
                                <svg viewBox="0 0 24 24" fill="currentColor"><path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/></svg>
                            </span>
                            <span>Google Scholar</span>
                        </a>
                    </li>
                </ul>
            </div>
        </aside>

        <main class="main-scroll">
            <section id="home" class="hero-intro">
                <div class="hero-content">
                    <div class="hero-text">
                        <h1 class="hero-title">
                            <span class="name">Ali K. AlShami</span>
                        </h1>
                        <p class="hero-role"><span class="hero-role-company">Postdoctoral Scientist at NEC Laboratories America, Inc. <a href="https://www.nec-labs.com/research/media-analytics/" target="_blank" rel="noopener noreferrer">Media Analytics Team</a></span></p>
                        <p class="hero-bio">Ph.D. in Computer Science with over six years of research experience in Artificial Intelligence (AI), Machine Learning (ML), Computer Vision (CV), and Natural Language Processing (NLP). In addition, five years of industry experience in systems engineering. My mission is to bridge recent advances in AI, CV, ML to develop advanced perception and prediction technologies for autonomous systems, with a focus on safety-critical environments and reliable decisionmaking. In particular my work focuses on design robust foundation models, new methodologies and benchmarks that address novelty problem in different scenarios, leveraging Vision Large Language Models (VLLMs) to enhance contextual understanding and improve automatic data engines to detect objects in complex environments for autonomous system applications.</p>
                    </div>
                </div>
            </section>

            <!-- Research Interests & News -->
            <section class="interests-news">
        <div class="container">
            <h2 class="block-heading">Research Interests</h2>
            <div class="research-interests-simple">
                <div class="research-item">
                    <p class="research-topic"><strong>Foundation Models &amp; Multimodal Reasoning</strong></p>
                    <p class="research-desc">VLMs, LLMs, and generative models for autonomous driving; vision-language perception and scene understanding; embodied AI for decision-making in AVs.</p>
                </div>
                <div class="research-item">
                    <p class="research-topic"><strong>Open-World &amp; Robust Autonomy</strong></p>
                    <p class="research-desc">Open-set recognition, OOD detection, and novel hazard avoidance; domain adaptation, transfer learning, and continual learning for robust autonomy.</p>
                </div>
                <div class="research-item">
                    <p class="research-topic"><strong>Prediction, Planning &amp; Interaction</strong></p>
                    <p class="research-desc">Motion forecasting, trajectory prediction, and behavior modeling; pedestrian intention and human-agent interaction; planning under uncertainty.</p>
                </div>
                <div class="research-item">
                    <p class="research-topic"><strong>Multimodal Perception &amp; Sensor Fusion</strong></p>
                    <p class="research-desc">Fusing camera, LiDAR, radar, and maps for scene understanding; spatio-temporal representation learning for dynamic environments.</p>
                </div>
                <div class="research-item">
                    <p class="research-topic"><strong>Generative Models &amp; Simulation</strong></p>
                    <p class="research-desc">Generative models for simulation, data augmentation, and scenario synthesis; synthetic data and sim-to-real transfer.</p>
                </div>
                <div class="research-item">
                    <p class="research-topic"><strong>Systems, Deployment &amp; Evaluation</strong></p>
                    <p class="research-desc">Efficient training, model compression, and edge deployment; real-time inference; benchmarks and safety-centric evaluation.</p>
                </div>
            </div>

            <h2 class="block-heading block-heading--news">News</h2>
            <div class="news-list">
                <article class="news-item news-item--highlight">
                    <span class="news-date">Feb 2026</span>
                    <p class="news-text">Started new role as a Postdoctoral Scientist at NEC Laboratories America, Inc.</p>
                </article>
                <article class="news-item news-item--highlight">
                    <span class="news-date">December 2025</span>
                    <p class="news-text">I successfully defended my <strong>Ph.D.</strong> </p>
                </article>
                <article class="news-item">
                    <span class="news-date">Jan 2026</span>
                    <p class="news-text">Our <strong>conference paper</strong> &ldquo;2COOOL: An Evaluation Benchmark for Generating Incident Reports on Out-of-Distribution Hazards in Autonomous Driving&rdquo; has been accepted to <em>WACV (January 2026)</em>.</p>
                </article>
                <article class="news-item">
                    <span class="news-date">Jan 2026</span>
                    <p class="news-text">Our <strong>conference paper</strong> &ldquo;GATEPose: A Graph Attention Transformer Enhanced with Pose and Orientation Angles for Pedestrian Crossing Intention Prediction&rdquo; has been accepted to <em>WACV (January 2026)</em>.</p>
                </article>
                <article class="news-item">
                    <span class="news-date">2026</span>
                    <p class="news-text">Our <strong>workshop proposal</strong> &ldquo;AUTOPILOT&rdquo; has been accepted and will take place at <em>CVPR 2026, Denver, Colorado</em>.</p>
                </article>
            </div>
        </div>
    </section>

            <!-- Footer -->
            <footer class="footer">
                <div class="container">
                    <p>&copy; 2026 Ali K. AlShami. All rights reserved.</p>
                </div>
            </footer>
        </main>
    </div>

    <script src="assets/js/main.js"></script>
</body>
</html>
